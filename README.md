# coolcode

A command-line AI coding assistant similar to aider, built with Node.js.

## Features

- ü§ñ Interactive chat interface for code modifications
- üìÅ Smart context management with auto-file detection
- üîç SEARCH/REPLACE block parsing and application
- üõ†Ô∏è Git integration and diff visualization
- üéØ Consolidated prompt management
- üîß Modular tool abstractions

## Installation

```bash
npm install
```

### Global Installation

To use coolcode from any directory, you can link it globally:

```bash
# Make the launcher executable and link globally
chmod +x bin/code-assistant
npm link

# Now you can use 'coolcode' from any directory
cd /path/to/your/project
coolcode chat
```

To unlink:

```bash
npm run unlink
# or manually: npm unlink -g
```

This creates a global symlink that allows you to run `coolcode` from any folder while working on different projects.

## Setup

Create a `.env` file in the project root and add your API keys:

```bash
# For Claude (default)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# For OpenAI (optional)
OPENAI_API_KEY=your_openai_api_key_here
```

Or set environment variables:

```bash
export ANTHROPIC_API_KEY="your-api-key-here"
export OPENAI_API_KEY="your-openai-api-key-here"
```

## Usage

### Interactive Chat Mode (Default)

Simply run coolcode to start interactive chat mode:

```bash
coolcode
```

Or with npm:

```bash
npm start
```

You can also explicitly use the chat command with options:

```bash
coolcode chat -f src/index.js package.json
```

Disable auto-apply and auto-commit:

```bash
coolcode chat --no-auto-apply --no-auto-commit
```

### Single Command Mode

```bash
coolcode apply "Add error handling to the main function"
```

With dry run:

```bash
coolcode apply "Refactor the user interface" --dry-run
```

Disable automatic behavior:

```bash
coolcode apply "Fix the bug" --no-auto-apply --no-auto-commit
```

### CLI Commands

In interactive mode:

- `help` - Show available commands
- `/add <files...>` - Add files to context
- `/remove <files...>` - Remove files from context
- `/context` - Show current context
- `/config` - Show current configuration
- `/set <key>=<value>` - Set configuration (autoApply, autoCommit, dryRun)
- `/clear` - Clear screen
- `/multi-start` - Enter multi-line input mode for complex requests
- `exit` or `quit` - Exit

#### Multi-line Input

For complex requests that span multiple lines (like detailed specifications, code examples, or long explanations), use the multi-line input mode:

1. Type `/multi-start` to begin multi-line input
2. Enter your request across multiple lines
3. Type `/multi-end` on an empty line to finish and send

This is especially useful when you need to include code snippets, detailed requirements, or structured requests in your message to the AI assistant.

### Default Behavior

By default, the assistant will:
1. **Automatically apply** code changes without asking for confirmation
2. **Automatically commit** changes to git with generated commit messages
3. Show diffs before applying changes

You can override this behavior with command-line flags or runtime configuration.

## Architecture

```
src/
‚îú‚îÄ‚îÄ index.js              # CLI entry point
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ CodeAssistant.js  # Main assistant orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ ContextManager.js # File context management
‚îÇ   ‚îî‚îÄ‚îÄ PromptManager.js  # Centralized prompt templates
‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îî‚îÄ‚îÄ LLMProvider.js    # OpenAI API integration
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ GitTool.js        # Git operations abstraction
‚îÇ   ‚îî‚îÄ‚îÄ DiffTool.js       # Diff generation and application
‚îî‚îÄ‚îÄ interfaces/
    ‚îî‚îÄ‚îÄ CLIInterface.js   # Interactive command line interface
```

## Configuration

The assistant supports various models and options:

```bash
# Use Claude (default)
coolcode --model claude-3-5-sonnet-20241022

# Use OpenAI
coolcode --model gpt-4
coolcode apply "Fix the bug" --files src/bug.js --model gpt-3.5-turbo
```

Supported models:
- **Claude**: `claude-3-5-sonnet-20241022`, `claude-3-haiku-20240307`, `claude-3-opus-20240229`
- **OpenAI**: `gpt-4`, `gpt-3.5-turbo`, `gpt-4-turbo`

## How It Works

The Code Assistant follows a structured flow from user input to code changes:

### 1. Input Processing
- **CLI Interface** (`CLIInterface.js`) captures user input and handles special commands (`/add`, `/remove`, etc.)
- For coding requests, the input is passed to the **Code Assistant** orchestrator

### 2. Code Assistant Orchestrator
The **Code Assistant** (`CodeAssistant.js`) serves as the central coordinator that:
- **Manages Configuration**: Tracks settings like `autoApply`, `autoCommit`, `dryRun`, and selected model
- **Coordinates Components**: Initializes and manages all tool instances (ContextManager, PromptLoader, LLMProvider, etc.)
- **Handles Lifecycle**: Ensures proper initialization order and manages the complete request-response cycle
- **Processes Requests**: Takes user input and orchestrates the entire flow from context assembly to change application
- **Error Handling**: Provides centralized error handling and logging throughout the process

### 3. Context Assembly & Prompt Structure
- **Context Manager** (`ContextManager.js`) maintains a map of loaded files and their contents
- **Prompt Loader** (`PromptLoader.js`) assembles the final prompt sent to the LLM with this structure:

```
[System Prompt - from prompts/system.txt]
- Instructions on how to act as an expert software developer
- Rules for SEARCH/REPLACE block format
- Guidelines for suggesting shell commands

[Context Information - from prompts/context.txt]
- Current project state: {totalFiles} files, {totalSize} bytes
- List of files in context:
  - file1.js
  - file2.py
  - ...

[File Contents]
=== path/to/file1.js ===
[actual file content]

=== path/to/file2.py ===
[actual file content]

[User Request]
{user's actual request}
```

This structured approach ensures the LLM has complete context about the codebase, clear instructions on response format, and the specific user request.

### 4. LLM Interaction
- **LLM Provider** (`LLMProvider.js`) sends the assembled prompt to the configured AI model (Claude/OpenAI)
- The provider handles API communication, model selection, and returns the model's response
- Supports both streaming and non-streaming responses depending on the interface

### 5. Response Parsing
The **Code Assistant** orchestrator parses the LLM response by:
- Using regex to extract SEARCH/REPLACE blocks from the response text
- Extracting file paths from lines preceding each code block
- Validating block structure and ensuring proper formatting
- Converting each block into a structured change object: `{ file, search, replace }`
- Handling edge cases like new file creation (empty search section)

### 6. Change Application
- **Diff Tool** (`DiffTool.js`) generates visual diffs showing what will change
- If not in dry-run mode, changes are applied by:
  - Reading existing file content
  - Performing search/replace operations
  - Writing updated content back to files
  - Creating new files when search section is empty

### 7. Git Integration
- **Git Tool** (`GitTool.js`) automatically stages changed files
- Generates descriptive commit messages based on user request and modified files
- Commits changes if auto-commit is enabled

### Internal Flow Diagram
```
User Input ‚Üí CLI Interface ‚Üí Code Assistant
                                ‚Üì
Context Manager ‚Üê Prompt Loader ‚Üê LLM Provider
                                ‚Üì
Response Parsing ‚Üí Diff Tool ‚Üí File System
                                ‚Üì
Git Tool ‚Üí Commit (if enabled)
```

This architecture ensures clean separation of concerns while maintaining a smooth user experience from request to applied changes.

## Development

Run in development mode with auto-reload:

```bash
npm run dev
```

## License

MIT
